{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models built for check-in in next month prediction (without the imbalanced dataset handling, but use class weight adjustment in the models)\n",
    "### Training dataset is up to end of Jun.2024 to see how models predict Jul.2024 check-in\n",
    "### The area under the precision-recall curve is around 5.6 which reflects the high recall, low precision nature for class 1 of previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in the necessary files\n",
    "import pandas as pd\n",
    "usr_segment = pd.read_csv(\"Jun_segmentation.csv\")\n",
    "features = pd.read_csv(\"features.csv\")\n",
    "openapp_recency = pd.read_csv(\"openapp_recency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.merge(usr_segment, features, on='user_sn', how='inner')\n",
    "merged_2 = pd.merge(merged_1, openapp_recency, on='user_sn', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_sn                        int64\n",
       "segmentation                   int64\n",
       "hourly_ci_number               int64\n",
       "ovn_ci_number                  int64\n",
       "day_ci_number                  int64\n",
       "total_paid_from_user           int64\n",
       "usr_cancel_num_3_months        int64\n",
       "hotel_cancel_num_3_months      int64\n",
       "g2j_cancel_num_3_months        int64\n",
       "no_show_num_3_months           int64\n",
       "recency                        int64\n",
       "average_ci_time_gap          float64\n",
       "std_ci_time_gap              float64\n",
       "open_app_num_3_months        float64\n",
       "mileage_used_num             float64\n",
       "current_mileage_point        float64\n",
       "search_num_3_months          float64\n",
       "review_num_3_months          float64\n",
       "avg_mark                     float64\n",
       "time_since_join                int64\n",
       "user_province                 object\n",
       "ci_num                         int64\n",
       "openapp_recency                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_sn                           0\n",
       "segmentation                      0\n",
       "hourly_ci_number                  0\n",
       "ovn_ci_number                     0\n",
       "day_ci_number                     0\n",
       "total_paid_from_user              0\n",
       "usr_cancel_num_3_months           0\n",
       "hotel_cancel_num_3_months         0\n",
       "g2j_cancel_num_3_months           0\n",
       "no_show_num_3_months              0\n",
       "recency                           0\n",
       "average_ci_time_gap               0\n",
       "std_ci_time_gap                   0\n",
       "open_app_num_3_months        117067\n",
       "mileage_used_num                239\n",
       "current_mileage_point         50134\n",
       "search_num_3_months          156746\n",
       "review_num_3_months          103449\n",
       "avg_mark                     103449\n",
       "time_since_join                   0\n",
       "user_province                     2\n",
       "ci_num                            0\n",
       "openapp_recency                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature explanation\n",
    "- segmentation: user segmentation by the month of train period\n",
    "\n",
    "- hourly_ci_number: user's number of hourly check-ins\n",
    "\n",
    "- ovn_ci_number: user's number of ovn check-ins \n",
    "\n",
    "- day_ci_number: user's number of day check-ins \n",
    "\n",
    "- total_paid_from_user: user's paid amount \n",
    "\n",
    "- usr_cancel_num_3_months: number of user cancels in the last 3 months in train period month\n",
    "\n",
    "- hotel_cancel_num_3_months: number of user cancels in the last 3 months in train period month\n",
    "\n",
    "- g2j_cancel_num_3_months: number of user cancels in the last 3 months  in train period month\n",
    "\n",
    "- no_show_num_3_months: number of noshow in the last 3 months in train period month\n",
    "\n",
    "- recency: the day difference between last check-in date before end of train period month and end of train period month\n",
    "\n",
    "- average_ci_time_gap: average day gap between check-ins\n",
    "\n",
    "- std_ci_time_gap: standard deviation of day gaps between check-ins\n",
    "\n",
    "- open_app_num_3_months: number of times user open app in the last 3 months in the train period month\n",
    "\n",
    "- mileage_used_num: number of times users used mileage points until end of train period month\n",
    "\n",
    "- current_mileage_point: mileage points of users by the end of train period month\n",
    "\n",
    "- search_num_3_months: number of times user search in the last 3 months in the train period month\n",
    "\n",
    "- review_num_3_months: number of times user give review in the last 3 months in the train period month\n",
    "\n",
    "- time_since_join: day gap between user's register date and end of train period month\n",
    "\n",
    "- user_province: Province of user\n",
    "\n",
    "- openapp_recency: the day difference between last open-app date before end of train period month and end of train period month\n",
    "\n",
    "- ci_num: number of user's check-ins in next month period (used to create target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill the null values with 0 for open_app_num_3_months, mileage_used_num, current_mileage_point\n",
    "## search_num_3_months, review_num_3_months, avg_mark\n",
    "merged_2[['open_app_num_3_months', 'mileage_used_num', 'current_mileage_point', \n",
    "          'review_num_3_months','search_num_3_months']] = merged_2[['open_app_num_3_months', 'mileage_used_num', 'current_mileage_point',\n",
    "                                                            'review_num_3_months','search_num_3_months',]].fillna(0).astype('int64')\n",
    "\n",
    "merged_2['avg_mark'] = merged_2['avg_mark'].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of records with time_since_join or average_ci_time_gap < 0 and records with user_province as null\n",
    "merged_2 = merged_2[merged_2['time_since_join']>=0]\n",
    "merged_2 = merged_2[merged_2['average_ci_time_gap']>=0]\n",
    "merged_2 = merged_2[merged_2['user_province'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265085, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_data = {\n",
    "    'segmentation': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'segmentation_text': ['New', 'Existing', 'Retention', 'Win-back', 'Churn', 'Drop', 'Dormant']\n",
    "}\n",
    "\n",
    "segment_df = pd.DataFrame(segment_data)\n",
    "\n",
    "# add the segementaion test\n",
    "cleaned_df = pd.merge(merged_2, segment_df, on='segmentation', how='left')\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target/dependent variable and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ci_or_not\n",
       "0    242262\n",
       "1     22823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['ci_or_not'] = cleaned_df['ci_num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "cleaned_df['ci_or_not'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# X = cleaned_df.drop(columns=['user_sn','segmentation', 'ci_or_not', 'ci_num'],axis=1)\n",
    "# y = cleaned_df['ci_or_not']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop(columns=['user_sn','segmentation', 'ci_or_not', 'ci_num'],axis=1)\n",
    "y = cleaned_df['ci_or_not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 19 numerical features : Index(['hourly_ci_number', 'ovn_ci_number', 'day_ci_number',\n",
      "       'total_paid_from_user', 'usr_cancel_num_3_months',\n",
      "       'hotel_cancel_num_3_months', 'g2j_cancel_num_3_months',\n",
      "       'no_show_num_3_months', 'recency', 'average_ci_time_gap',\n",
      "       'std_ci_time_gap', 'open_app_num_3_months', 'mileage_used_num',\n",
      "       'current_mileage_point', 'search_num_3_months', 'review_num_3_months',\n",
      "       'avg_mark', 'time_since_join', 'openapp_recency'],\n",
      "      dtype='object')\n",
      "\n",
      "We have 2 categorical features : Index(['user_province', 'segmentation_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "print('We have {} numerical features : {}'.format(len(num_features), num_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(cat_features), cat_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and train for Tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "oh_transformer = OneHotEncoder(sparse_output=False)\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),    \n",
    "        ('pass', 'passthrough', num_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV 2/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=10, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=50;, score=0.547 total time=   4.6s\n",
      "[CV 1/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=10, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=50;, score=0.553 total time=   4.7s\n",
      "[CV 3/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=10, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=50;, score=0.554 total time=   4.6s\n",
      "[CV 1/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=50, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=150;, score=0.570 total time=  21.6s\n",
      "[CV 2/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=50, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=150;, score=0.565 total time=  21.7s\n",
      "[CV 3/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=50, classifier__max_features=log2, classifier__min_samples_leaf=4, classifier__min_samples_split=10, classifier__n_estimators=150;, score=0.566 total time=  20.9s\n",
      "[CV 1/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=40, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150;, score=0.573 total time=  29.2s\n",
      "[CV 2/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=40, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150;, score=0.570 total time=  29.4s\n",
      "[CV 3/3] END classifier__bootstrap=True, classifier__class_weight={0: 1, 1: 10}, classifier__max_depth=40, classifier__max_features=sqrt, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150;, score=0.568 total time=  28.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trihoang/Desktop/g2j_project/venv/lib/python3.11/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__n_estimators': 150, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 2, 'classifier__max_features': 'sqrt', 'classifier__max_depth': 40, 'classifier__class_weight': {0: 1, 1: 10}, 'classifier__bootstrap': True}\n",
      "Best PRC AUC score: 0.5705624975013207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define a custom PRC AUC scoring function\n",
    "def prc_auc_score(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Create a custom scorer using make_scorer\n",
    "prc_auc_scorer = make_scorer(prc_auc_score, response_method='predict_proba')\n",
    "\n",
    "# Define the model and parameter grid\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Apply the preprocessing steps\n",
    "    ('classifier', RandomForestClassifier())  # Fit the model\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "    'classifier__class_weight': [{0: 1, 1: 5}, {0: 1, 1: 10}]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV with the custom scorer\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=3,\n",
    "    scoring=prc_auc_scorer,\n",
    "    cv=stratified_kfold,\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best PRC AUC score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV 1/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.562 total time= 1.7min\n",
      "[CV 2/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.555 total time= 1.7min\n",
      "[CV 3/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.562 total time= 1.7min\n",
      "[CV 1/3] END classifier__n_estimators=150, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.562 total time= 5.2min\n",
      "[CV 2/3] END classifier__n_estimators=150, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.557 total time= 5.2min\n",
      "[CV 1/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=auto;, score=0.563 total time= 5.5min\n",
      "[CV 3/3] END classifier__n_estimators=150, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=all;, score=0.562 total time= 9.0min\n",
      "[CV 2/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=auto;, score=0.558 total time= 1.9min\n",
      "[CV 3/3] END classifier__n_estimators=50, classifier__random_state=42, classifier__replacement=True, classifier__sampling_strategy=auto;, score=0.561 total time= 1.5min\n",
      "Best parameters: {'classifier__sampling_strategy': 'auto', 'classifier__replacement': True, 'classifier__random_state': 42, 'classifier__n_estimators': 50}\n",
      "Best PRC AUC score: 0.5606092338908925\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define a custom PRC AUC scoring function\n",
    "def prc_auc_score(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Create a custom scorer using make_scorer\n",
    "prc_auc_scorer = make_scorer(prc_auc_score, response_method='predict_proba')\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],  # Number of base estimators in the ensemble\n",
    "    'classifier__sampling_strategy': ['auto', 'all'],  # Strategy for sampling\n",
    "    'classifier__replacement': [True],  # Whether sampling should be with replacement\n",
    "    'classifier__random_state': [42]  # Seed for random number generator\n",
    "}\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Apply the preprocessing steps\n",
    "    ('classifier', EasyEnsembleClassifier())  # Fit the model\n",
    "])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=3,\n",
    "    scoring=prc_auc_scorer,\n",
    "    cv=stratified_kfold,\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best PRC AUC score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and train for non-tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "oh_transformer = OneHotEncoder()\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "         (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold PRC AUC score: 0.5477\n",
      "Fold PRC AUC score: 0.5389\n",
      "Fold PRC AUC score: 0.5389\n",
      "Fold PRC AUC score: 0.5344\n",
      "Fold PRC AUC score: 0.5338\n",
      "PRC AUC score: 0.5388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "class_weight = {0: 1.0, 1: 5.0} \n",
    "\n",
    "# Define the ANN model architecture\n",
    "def build_ann():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(89,)))  # Specify the input shape using Input layer\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model with KerasClassifier\n",
    "ann_model = KerasClassifier(model=build_ann, verbose=0)\n",
    "\n",
    "# Create the pipeline with preprocessing and the Keras model\n",
    "Ann_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Apply the preprocessing steps\n",
    "    ('classifier', ann_model)  # Fit the model\n",
    "])\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=32)\n",
    "\n",
    "# Store scores for each fold\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    Ann_pipeline.fit(X_train, y_train, classifier__class_weight=class_weight)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_proba = Ann_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evaluate the model\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auc_score = auc(recall, precision)\n",
    "    scores.append(auc_score)\n",
    "    print(f\"Fold PRC AUC score: {auc_score:.4f}\")\n",
    "\n",
    "# Print the average accuracy across all folds\n",
    "print(f\"PRC AUC score: {np.mean(scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
