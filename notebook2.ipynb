{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models built for check-in in next month prediction (without the imbalanced dataset handling, but use class weight adjustment in the models)\n",
    "### Training dataset is up to end of Jun.2024 to see how models predict Jul.2024 check-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in the necessary files\n",
    "import pandas as pd\n",
    "usr_segment = pd.read_csv(\"Jun_segmentation.csv\")\n",
    "features = pd.read_csv(\"features.csv\")\n",
    "openapp_recency = pd.read_csv(\"openapp_recency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_1 = pd.merge(usr_segment, features, on='user_sn', how='inner')\n",
    "merged_2 = pd.merge(merged_1, openapp_recency, on='user_sn', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_sn                        int64\n",
       "segmentation                   int64\n",
       "hourly_ci_number               int64\n",
       "ovn_ci_number                  int64\n",
       "day_ci_number                  int64\n",
       "total_paid_from_user           int64\n",
       "usr_cancel_num_3_months        int64\n",
       "hotel_cancel_num_3_months      int64\n",
       "g2j_cancel_num_3_months        int64\n",
       "no_show_num_3_months           int64\n",
       "recency                        int64\n",
       "average_ci_time_gap          float64\n",
       "std_ci_time_gap              float64\n",
       "open_app_num_3_months        float64\n",
       "mileage_used_num             float64\n",
       "current_mileage_point        float64\n",
       "search_num_3_months          float64\n",
       "review_num_3_months          float64\n",
       "avg_mark                     float64\n",
       "time_since_join                int64\n",
       "user_province                 object\n",
       "ci_num                         int64\n",
       "openapp_recency                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_sn                           0\n",
       "segmentation                      0\n",
       "hourly_ci_number                  0\n",
       "ovn_ci_number                     0\n",
       "day_ci_number                     0\n",
       "total_paid_from_user              0\n",
       "usr_cancel_num_3_months           0\n",
       "hotel_cancel_num_3_months         0\n",
       "g2j_cancel_num_3_months           0\n",
       "no_show_num_3_months              0\n",
       "recency                           0\n",
       "average_ci_time_gap               0\n",
       "std_ci_time_gap                   0\n",
       "open_app_num_3_months        117067\n",
       "mileage_used_num                239\n",
       "current_mileage_point         50134\n",
       "search_num_3_months          156746\n",
       "review_num_3_months          103449\n",
       "avg_mark                     103449\n",
       "time_since_join                   0\n",
       "user_province                     2\n",
       "ci_num                            0\n",
       "openapp_recency                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature explanation\n",
    "- segmentation: user segmentation by the month of train period\n",
    "\n",
    "- hourly_ci_number: user's number of hourly check-ins\n",
    "\n",
    "- ovn_ci_number: user's number of ovn check-ins \n",
    "\n",
    "- day_ci_number: user's number of day check-ins \n",
    "\n",
    "- total_paid_from_user: user's paid amount \n",
    "\n",
    "- usr_cancel_num_3_months: number of user cancels in the last 3 months in train period month\n",
    "\n",
    "- hotel_cancel_num_3_months: number of user cancels in the last 3 months in train period month\n",
    "\n",
    "- g2j_cancel_num_3_months: number of user cancels in the last 3 months  in train period month\n",
    "\n",
    "- no_show_num_3_months: number of noshow in the last 3 months in train period month\n",
    "\n",
    "- recency: the day difference between last check-in date before end of train period month and end of train period month\n",
    "\n",
    "- average_ci_time_gap: average day gap between check-ins\n",
    "\n",
    "- std_ci_time_gap: standard deviation of day gaps between check-ins\n",
    "\n",
    "- open_app_num_3_months: number of times user open app in the last 3 months in the train period month\n",
    "\n",
    "- mileage_used_num: number of times users used mileage points until end of train period month\n",
    "\n",
    "- current_mileage_point: mileage points of users by the end of train period month\n",
    "\n",
    "- search_num_3_months: number of times user search in the last 3 months in the train period month\n",
    "\n",
    "- review_num_3_months: number of times user give review in the last 3 months in the train period month\n",
    "\n",
    "- time_since_join: day gap between user's register date and end of train period month\n",
    "\n",
    "- user_province: Province of user\n",
    "\n",
    "- openapp_recency: the day difference between last open-app date before end of train period month and end of train period month\n",
    "\n",
    "- ci_num: number of user's check-ins in next month period (used to create target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill the null values with 0 for open_app_num_3_months, mileage_used_num, current_mileage_point\n",
    "## search_num_3_months, review_num_3_months, avg_mark\n",
    "merged_2[['open_app_num_3_months', 'mileage_used_num', 'current_mileage_point', \n",
    "          'review_num_3_months','search_num_3_months']] = merged_2[['open_app_num_3_months', 'mileage_used_num', 'current_mileage_point',\n",
    "                                                            'review_num_3_months','search_num_3_months',]].fillna(0).astype('int64')\n",
    "\n",
    "merged_2['avg_mark'] = merged_2['avg_mark'].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of records with time_since_join or average_ci_time_gap < 0 and records with user_province as null\n",
    "merged_2 = merged_2[merged_2['time_since_join']>=0]\n",
    "merged_2 = merged_2[merged_2['average_ci_time_gap']>=0]\n",
    "merged_2 = merged_2[merged_2['user_province'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265085, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_data = {\n",
    "    'segmentation': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'segmentation_text': ['New', 'Existing', 'Retention', 'Win-back', 'Churn', 'Drop', 'Dormant']\n",
    "}\n",
    "\n",
    "segment_df = pd.DataFrame(segment_data)\n",
    "\n",
    "# add the segementaion test\n",
    "cleaned_df = pd.merge(merged_2, segment_df, on='segmentation', how='left')\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target/dependent variable and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ci_or_not\n",
       "0    242262\n",
       "1     22823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df['ci_or_not'] = cleaned_df['ci_num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "cleaned_df['ci_or_not'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = cleaned_df.drop(columns=['user_sn','segmentation', 'ci_or_not', 'ci_num'],axis=1)\n",
    "y = cleaned_df['ci_or_not']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 19 numerical features : Index(['hourly_ci_number', 'ovn_ci_number', 'day_ci_number',\n",
      "       'total_paid_from_user', 'usr_cancel_num_3_months',\n",
      "       'hotel_cancel_num_3_months', 'g2j_cancel_num_3_months',\n",
      "       'no_show_num_3_months', 'recency', 'average_ci_time_gap',\n",
      "       'std_ci_time_gap', 'open_app_num_3_months', 'mileage_used_num',\n",
      "       'current_mileage_point', 'search_num_3_months', 'review_num_3_months',\n",
      "       'avg_mark', 'time_since_join', 'openapp_recency'],\n",
      "      dtype='object')\n",
      "\n",
      "We have 2 categorical features : Index(['user_province', 'segmentation_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X_train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "print('We have {} numerical features : {}'.format(len(num_features), num_features))\n",
    "print('\\nWe have {} categorical features : {}'.format(len(cat_features), cat_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and train for Tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "oh_transformer = OneHotEncoder()\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),    \n",
    "        ('pass', 'passthrough', num_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94     48452\n",
      "           1       0.42      0.71      0.53      4565\n",
      "\n",
      "    accuracy                           0.89     53017\n",
      "   macro avg       0.70      0.81      0.73     53017\n",
      "weighted avg       0.92      0.89      0.90     53017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class_weight = {0: 1, 1: 10} \n",
    "rf = RandomForestClassifier(n_estimators=150, min_samples_split=7, min_samples_leaf=5, criterion='gini',\n",
    "                            max_features='sqrt', max_samples=0.8, bootstrap=True, class_weight=class_weight, random_state=2)\n",
    "\n",
    "processed_X_train = preprocessor.fit_transform(X_train)\n",
    "processed_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "rf.fit(processed_X_train, y_train) \n",
    "\n",
    "y_test_pred = rf.predict(processed_X_test)\n",
    "\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(\"Test Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good precision and recall for 0 (no checkin in next month) class\n",
    "##### Precison of 0.42 for 1 (checkin in next month) class -> out of all next month check-in predictions, model gets 42% of predictions are correct\n",
    "##### Recall of 0.71 for 1 (checkin in next month) class -> out of all next month actual check-ins, model gets 71% of actual check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90     48452\n",
      "           1       0.31      0.83      0.46      4565\n",
      "\n",
      "    accuracy                           0.83     53017\n",
      "   macro avg       0.65      0.83      0.68     53017\n",
      "weighted avg       0.92      0.83      0.86     53017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "processed_X_train = preprocessor.fit_transform(X_train)\n",
    "processed_X_test = preprocessor.transform(X_test)\n",
    "# Initialize and train the EasyEnsemble model\n",
    "eec = EasyEnsembleClassifier(n_estimators=150, random_state=42)\n",
    "eec.fit(processed_X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = eec.predict(processed_X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good precision and recall for 0 (no checkin in next month) class\n",
    "##### Precison of 0.31 for 1 (checkin in next month) class -> out of all next month check-in predictions, model gets 31% of predictions are correct\n",
    "##### Recall of 0.83 for 1 (checkin in next month) class -> out of all next month actual check-ins, model gets 83% of actual check-ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and train for non-tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "oh_transformer = OneHotEncoder()\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "         (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90     48452\n",
      "           1       0.31      0.81      0.45      4565\n",
      "\n",
      "    accuracy                           0.83     53017\n",
      "   macro avg       0.65      0.82      0.67     53017\n",
      "weighted avg       0.92      0.83      0.86     53017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class_weight = {0: 1, 1: 10} \n",
    "model = LogisticRegression(class_weight=class_weight, random_state=42)\n",
    "\n",
    "processed_X_train = preprocessor.fit_transform(X_train)\n",
    "processed_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "model.fit(processed_X_train, y_train)\n",
    "\n",
    "y_test_pred = model.predict(processed_X_test)\n",
    "\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(\"Test Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good precision and recall for 0 (no checkin in next month) class\n",
    "##### Precison of 0.31 for 1 (checkin in next month) class -> out of all next month check-in predictions, model gets 31% of predictions are correct\n",
    "##### Recall of 0.81 for 1 (checkin in next month) class -> out of all next month actual check-ins, model gets 81% of actual check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trihoang/Desktop/g2j_project/venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 492us/step - accuracy: 0.8649 - loss: 0.5628 - val_accuracy: 0.8764 - val_loss: 0.2712\n",
      "Epoch 2/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 466us/step - accuracy: 0.8793 - loss: 0.5127 - val_accuracy: 0.8888 - val_loss: 0.2537\n",
      "Epoch 3/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 468us/step - accuracy: 0.8827 - loss: 0.5048 - val_accuracy: 0.8915 - val_loss: 0.2445\n",
      "Epoch 4/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470us/step - accuracy: 0.8827 - loss: 0.5026 - val_accuracy: 0.8866 - val_loss: 0.2522\n",
      "Epoch 5/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 473us/step - accuracy: 0.8839 - loss: 0.4982 - val_accuracy: 0.8919 - val_loss: 0.2458\n",
      "Epoch 6/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470us/step - accuracy: 0.8840 - loss: 0.5009 - val_accuracy: 0.8913 - val_loss: 0.2482\n",
      "Epoch 7/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 468us/step - accuracy: 0.8843 - loss: 0.4971 - val_accuracy: 0.8918 - val_loss: 0.2478\n",
      "Epoch 8/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 476us/step - accuracy: 0.8851 - loss: 0.4951 - val_accuracy: 0.8877 - val_loss: 0.2581\n",
      "Epoch 9/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 491us/step - accuracy: 0.8855 - loss: 0.4962 - val_accuracy: 0.8902 - val_loss: 0.2524\n",
      "Epoch 10/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 480us/step - accuracy: 0.8862 - loss: 0.4938 - val_accuracy: 0.8913 - val_loss: 0.2496\n",
      "Epoch 11/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 495us/step - accuracy: 0.8862 - loss: 0.4929 - val_accuracy: 0.8919 - val_loss: 0.2515\n",
      "Epoch 12/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 476us/step - accuracy: 0.8862 - loss: 0.4918 - val_accuracy: 0.8913 - val_loss: 0.2496\n",
      "Epoch 13/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 478us/step - accuracy: 0.8859 - loss: 0.4910 - val_accuracy: 0.8913 - val_loss: 0.2557\n",
      "Epoch 14/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 471us/step - accuracy: 0.8876 - loss: 0.4932 - val_accuracy: 0.8951 - val_loss: 0.2453\n",
      "Epoch 15/15\n",
      "\u001b[1m5302/5302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470us/step - accuracy: 0.8878 - loss: 0.4899 - val_accuracy: 0.8904 - val_loss: 0.2567\n",
      "\u001b[1m1657/1657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94     48452\n",
      "           1       0.42      0.71      0.53      4565\n",
      "\n",
      "    accuracy                           0.89     53017\n",
      "   macro avg       0.70      0.81      0.73     53017\n",
      "weighted avg       0.92      0.89      0.90     53017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "\n",
    "\n",
    "class_weight = {0: 1.0, 1: 5.0} \n",
    "\n",
    "processed_X_train = preprocessor.fit_transform(X_train)\n",
    "processed_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Stratified split into training and validation sets\n",
    "nn_X_train, nn_X_val, nn_y_train, nn_y_val = train_test_split(processed_X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# Convert the data to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((nn_X_train.toarray(), nn_y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((nn_X_val.toarray(), nn_y_val))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Define a simple ANN model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', kernel_initializer='glorot_uniform', input_shape=(nn_X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_initializer='glorot_uniform'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the stratified validation set\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=15, class_weight=class_weight)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(processed_X_test)\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to class labels\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good precision and recall for 0 (no checkin in next month) class\n",
    "##### Precison of 0.42 for 1 (checkin in next month) class -> out of all next month check-in predictions, model gets 42% of predictions are correct\n",
    "##### Recall of 0.71 for 1 (checkin in next month) class -> out of all next month actual check-ins, model gets 71% of actual check-ins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
